#!/usr/bin/env python3
"""
Florence-2 å¿«é€Ÿæµ‹è¯• - æœ€ç®€å•çš„ä½¿ç”¨ç¤ºä¾‹
åªéœ€10è¡Œä»£ç å³å¯è¿è¡ŒOCRï¼
"""

from transformers import AutoProcessor, AutoModelForCausalLM
from PIL import Image
import sys

# 1. åŠ è½½æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½ï¼‰
model = AutoModelForCausalLM.from_pretrained("microsoft/Florence-2-large", trust_remote_code=True)
processor = AutoProcessor.from_pretrained("microsoft/Florence-2-large", trust_remote_code=True)

# 2. è¯»å–å›¾åƒ
image_path = sys.argv[1] if len(sys.argv) > 1 else "test.jpg"
image = Image.open(image_path)

# 3. æ‰§è¡ŒOCR
inputs = processor(text="<OCR>", images=image, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=1024)
text = processor.batch_decode(outputs, skip_special_tokens=False)[0]
result = processor.post_process_generation(text, task="<OCR>", image_size=image.size)

# 4. æ˜¾ç¤ºç»“æœ
print("OCRç»“æœ:")
print(result['OCR'] if 'OCR' in result else result)

# 5. ä¿å­˜ç»“æœ
import os
from pathlib import Path
output_dir = Path("/workspace/output/florence2")
output_dir.mkdir(exist_ok=True)
output_file = output_dir / f"{os.path.basename(image_path).split('.')[0]}_ocr.txt"
with open(output_file, 'w', encoding='utf-8') as f:
    f.write(result['OCR'] if 'OCR' in result else str(result))
print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_file}")

# ===========================================================
# ä»¥ä¸‹æ˜¯ä¸€äº›å…¶ä»–å¸¸ç”¨åŠŸèƒ½çš„ç¤ºä¾‹
# ===========================================================

def test_all_features(image_path):
    """æµ‹è¯•Florence-2çš„å„ç§åŠŸèƒ½"""
    from transformers import AutoProcessor, AutoModelForCausalLM
    from PIL import Image
    import torch
    
    # åŠ è½½æ¨¡å‹
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = AutoModelForCausalLM.from_pretrained(
        "microsoft/Florence-2-large",
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        trust_remote_code=True
    ).to(device)
    processor = AutoProcessor.from_pretrained("microsoft/Florence-2-large", trust_remote_code=True)
    
    # åŠ è½½å›¾åƒ
    image = Image.open(image_path)
    
    # å®šä¹‰æ‰€æœ‰ä»»åŠ¡
    tasks = {
        "OCRæ–‡å­—è¯†åˆ«": "<OCR>",
        "å›¾åƒæè¿°": "<CAPTION>",
        "è¯¦ç»†æè¿°": "<DETAILED_CAPTION>",
        "ç›®æ ‡æ£€æµ‹": "<OD>",
        "å¸¦åŒºåŸŸçš„OCR": "<OCR_WITH_REGION>",
        "å¯†é›†åŒºåŸŸæè¿°": "<DENSE_REGION_CAPTION>",
    }
    
    print(f"æµ‹è¯•å›¾åƒ: {image_path}")
    print("=" * 60)
    
    # æµ‹è¯•æ¯ä¸ªä»»åŠ¡
    for task_name, prompt in tasks.items():
        print(f"\n{task_name}:")
        print("-" * 40)
        
        # å¤„ç†
        inputs = processor(text=prompt, images=image, return_tensors="pt").to(device)
        generated_ids = model.generate(
            input_ids=inputs["input_ids"],
            pixel_values=inputs["pixel_values"],
            max_new_tokens=1024,
            do_sample=False,
            num_beams=3
        )
        
        # è§£ç 
        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]
        parsed_answer = processor.post_process_generation(
            generated_text,
            task=prompt,
            image_size=(image.width, image.height)
        )
        
        # æ˜¾ç¤ºç»“æœ
        if isinstance(parsed_answer, dict):
            for key, value in parsed_answer.items():
                if isinstance(value, str):
                    # æ–‡æœ¬ç»“æœ
                    preview = value[:200] + "..." if len(value) > 200 else value
                    print(f"{key}: {preview}")
                elif isinstance(value, dict):
                    # ç»“æ„åŒ–ç»“æœï¼ˆå¦‚æ£€æµ‹æ¡†ï¼‰
                    if 'bboxes' in value:
                        print(f"æ£€æµ‹åˆ° {len(value['bboxes'])} ä¸ªå¯¹è±¡")
                    elif 'quad_boxes' in value:
                        print(f"æ£€æµ‹åˆ° {len(value['quad_boxes'])} ä¸ªæ–‡æœ¬åŒºåŸŸ")
                    else:
                        print(f"{key}: {value}")
                elif isinstance(value, list):
                    print(f"{key}: {len(value)} é¡¹")
        else:
            print(str(parsed_answer)[:200])

def visual_qa_demo(image_path):
    """è§†è§‰é—®ç­”æ¼”ç¤º"""
    from transformers import AutoProcessor, AutoModelForCausalLM
    from PIL import Image
    
    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained("microsoft/Florence-2-large", trust_remote_code=True)
    processor = AutoProcessor.from_pretrained("microsoft/Florence-2-large", trust_remote_code=True)
    
    # åŠ è½½å›¾åƒ
    image = Image.open(image_path)
    
    print(f"ğŸ“¸ å›¾åƒ: {image_path}")
    print("è¾“å…¥é—®é¢˜ (è¾“å…¥ 'quit' é€€å‡º):\n")
    
    while True:
        question = input("â“ é—®é¢˜: ")
        if question.lower() == 'quit':
            break
        
        # VQAå¤„ç†
        prompt = f"<VQA> {question}"
        inputs = processor(text=prompt, images=image, return_tensors="pt")
        outputs = model.generate(**inputs, max_new_tokens=256)
        text = processor.batch_decode(outputs, skip_special_tokens=False)[0]
        answer = processor.post_process_generation(text, task=prompt, image_size=image.size)
        
        # æ˜¾ç¤ºç­”æ¡ˆ
        if isinstance(answer, dict) and 'VQA' in answer:
            print(f"ğŸ’¡ ç­”æ¡ˆ: {answer['VQA']}\n")
        else:
            print(f"ğŸ’¡ ç­”æ¡ˆ: {answer}\n")

# ===========================================================
# æ‰¹é‡OCRç¤ºä¾‹
# ===========================================================

def batch_ocr(folder_path):
    """æ‰¹é‡OCRå¤„ç†"""
    from pathlib import Path
    from transformers import AutoProcessor, AutoModelForCausalLM
    from PIL import Image
    
    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained("microsoft/Florence-2-large", trust_remote_code=True)
    processor = AutoProcessor.from_pretrained("microsoft/Florence-2-large", trust_remote_code=True)
    
    # æŸ¥æ‰¾æ‰€æœ‰å›¾åƒ
    folder = Path(folder_path)
    images = list(folder.glob("*.jpg")) + list(folder.glob("*.png"))
    
    print(f"æ‰¾åˆ° {len(images)} ä¸ªå›¾åƒ")
    
    # å¤„ç†æ¯ä¸ªå›¾åƒ
    for img_path in images:
        print(f"\nå¤„ç†: {img_path.name}")
        
        # OCR
        image = Image.open(img_path)
        inputs = processor(text="<OCR>", images=image, return_tensors="pt")
        outputs = model.generate(**inputs, max_new_tokens=1024)
        text = processor.batch_decode(outputs, skip_special_tokens=False)[0]
        result = processor.post_process_generation(text, task="<OCR>", image_size=image.size)
        
        # ä¿å­˜ç»“æœ
        import os
        ocr_text = result.get('OCR', str(result))
        output_dir = Path("/workspace/output/florence2")
        output_dir.mkdir(exist_ok=True)
        output_file = output_dir / f"{img_path.stem}.txt"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(ocr_text)
        
        print(f"âœ… ä¿å­˜åˆ°: {output_file}")
        print(f"   å­—ç¬¦æ•°: {len(ocr_text)}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--demo":
        # è¿è¡Œå®Œæ•´æ¼”ç¤º
        if len(sys.argv) > 2:
            test_all_features(sys.argv[2])
        else:
            print("ç”¨æ³•: python florence2_quick.py --demo <å›¾åƒè·¯å¾„>")
    elif len(sys.argv) > 1 and sys.argv[1] == "--vqa":
        # è¿è¡ŒVQAæ¼”ç¤º
        if len(sys.argv) > 2:
            visual_qa_demo(sys.argv[2])
        else:
            print("ç”¨æ³•: python florence2_quick.py --vqa <å›¾åƒè·¯å¾„>")
    elif len(sys.argv) > 1 and sys.argv[1] == "--batch":
        # æ‰¹é‡å¤„ç†
        if len(sys.argv) > 2:
            batch_ocr(sys.argv[2])
        else:
            print("ç”¨æ³•: python florence2_quick.py --batch <æ–‡ä»¶å¤¹è·¯å¾„>")
    else:
        # é»˜è®¤ï¼šç®€å•OCR
        print("""
Florence-2 å¿«é€Ÿæµ‹è¯•å·¥å…·
========================
ç”¨æ³•:
  python florence2_quick.py <å›¾åƒ>           # OCRè¯†åˆ«
  python florence2_quick.py --demo <å›¾åƒ>    # æµ‹è¯•æ‰€æœ‰åŠŸèƒ½
  python florence2_quick.py --vqa <å›¾åƒ>     # è§†è§‰é—®ç­”
  python florence2_quick.py --batch <æ–‡ä»¶å¤¹>  # æ‰¹é‡OCR
        """)
