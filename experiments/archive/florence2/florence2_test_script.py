#!/usr/bin/env python3
"""
Florence-2 å¤šåŠŸèƒ½è§†è§‰æ¨¡å‹æµ‹è¯•è„šæœ¬
æ”¯æŒOCRã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒæè¿°ã€è§†è§‰é—®ç­”ç­‰å¤šç§ä»»åŠ¡
"""

import torch
from PIL import Image, ImageDraw, ImageFont
from transformers import AutoProcessor, AutoModelForCausalLM
import numpy as np
from pathlib import Path
import json
import time
from typing import Dict, List, Tuple, Optional
import re

class Florence2Tester:
    """Florence-2 æ¨¡å‹æµ‹è¯•å™¨"""
    
    # æ”¯æŒçš„ä»»åŠ¡ç±»å‹å’Œå¯¹åº”çš„æç¤ºè¯
    TASK_PROMPTS = {
        # OCRç›¸å…³ä»»åŠ¡
        'ocr': '<OCR>',
        'ocr_with_region': '<OCR_WITH_REGION>',
        
        # å›¾åƒæè¿°ä»»åŠ¡
        'caption': '<CAPTION>',
        'detailed_caption': '<DETAILED_CAPTION>',
        'more_detailed_caption': '<MORE_DETAILED_CAPTION>',
        
        # ç›®æ ‡æ£€æµ‹ä»»åŠ¡
        'object_detection': '<OD>',
        'dense_region_caption': '<DENSE_REGION_CAPTION>',
        'region_proposal': '<REGION_PROPOSAL>',
        
        # åˆ†å‰²ä»»åŠ¡
        'referring_expression_segmentation': '<REFERRING_EXPRESSION_SEGMENTATION>',
        'region_to_segmentation': '<REGION_TO_SEGMENTATION>',
        'open_vocabulary_detection': '<OPEN_VOCABULARY_DETECTION>',
        'region_to_category': '<REGION_TO_CATEGORY>',
        'region_to_description': '<REGION_TO_DESCRIPTION>',
        
        # è§†è§‰é—®ç­”
        'vqa': '',  # VQAéœ€è¦ç”¨æˆ·æä¾›é—®é¢˜
        
        # æ›´å¤šä»»åŠ¡
        'caption_to_phrase_grounding': '<CAPTION_TO_PHRASE_GROUNDING>',
    }
    
    def __init__(self, model_id="microsoft/Florence-2-large", device=None):
        """
        åˆå§‹åŒ–Florence-2æ¨¡å‹
        
        Args:
            model_id: æ¨¡å‹IDï¼Œå¯é€‰ï¼š
                - microsoft/Florence-2-base (0.23Bå‚æ•°)
                - microsoft/Florence-2-large (0.77Bå‚æ•°)
            device: è¿è¡Œè®¾å¤‡
        """
        print(f"ğŸš€ æ­£åœ¨åŠ è½½ Florence-2 æ¨¡å‹: {model_id}")
        print("é¦–æ¬¡åŠ è½½éœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        
        # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
        self.model = AutoModelForCausalLM.from_pretrained(
            model_id, 
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            trust_remote_code=True
        )
        self.processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)
        
        # è®¾ç½®è®¾å¤‡
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
            
        self.model.to(self.device)
        self.model.eval()
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_tasks': 0,
            'successful_tasks': 0,
            'failed_tasks': 0,
            'total_time': 0
        }
    
    def run_task(self, image: Image.Image, task: str, text_input: str = "") -> Dict:
        """
        æ‰§è¡Œå•ä¸ªä»»åŠ¡
        
        Args:
            image: PILå›¾åƒå¯¹è±¡
            task: ä»»åŠ¡ç±»å‹
            text_input: é¢å¤–çš„æ–‡æœ¬è¾“å…¥ï¼ˆç”¨äºVQAç­‰ä»»åŠ¡ï¼‰
        
        Returns:
            åŒ…å«ç»“æœçš„å­—å…¸
        """
        try:
            start_time = time.time()
            
            # è·å–ä»»åŠ¡æç¤ºè¯
            if task not in self.TASK_PROMPTS:
                raise ValueError(f"ä¸æ”¯æŒçš„ä»»åŠ¡: {task}")
            
            prompt = self.TASK_PROMPTS[task]
            
            # å¯¹äºVQAä»»åŠ¡ï¼Œéœ€è¦æ·»åŠ é—®é¢˜
            if task == 'vqa':
                if not text_input:
                    text_input = "What is in this image?"  # é»˜è®¤é—®é¢˜
                prompt = f"<VQA> {text_input}"
            
            # å¯¹äºéœ€è¦é¢å¤–è¾“å…¥çš„ä»»åŠ¡
            if task in ['open_vocabulary_detection', 'caption_to_phrase_grounding']:
                if text_input:
                    prompt = f"{prompt}{text_input}"
            
            # é¢„å¤„ç†è¾“å…¥
            inputs = self.processor(
                text=prompt, 
                images=image, 
                return_tensors="pt"
            ).to(self.device)
            
            # ç”Ÿæˆè¾“å‡º
            with torch.no_grad():
                generated_ids = self.model.generate(
                    input_ids=inputs["input_ids"],
                    pixel_values=inputs["pixel_values"],
                    max_new_tokens=1024,
                    do_sample=False,
                    num_beams=3,
                )
            
            # è§£ç è¾“å‡º
            generated_text = self.processor.batch_decode(
                generated_ids, 
                skip_special_tokens=False
            )[0]
            
            # è§£æç»“æœ
            parsed_answer = self.processor.post_process_generation(
                generated_text, 
                task=prompt, 
                image_size=(image.width, image.height)
            )
            
            elapsed_time = time.time() - start_time
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats['total_tasks'] += 1
            self.stats['successful_tasks'] += 1
            self.stats['total_time'] += elapsed_time
            
            return {
                'task': task,
                'prompt': prompt,
                'raw_output': generated_text,
                'parsed_output': parsed_answer,
                'success': True,
                'time': elapsed_time
            }
            
        except Exception as e:
            self.stats['total_tasks'] += 1
            self.stats['failed_tasks'] += 1
            
            return {
                'task': task,
                'success': False,
                'error': str(e)
            }
    
    def visualize_detection(self, image: Image.Image, detection_result: Dict) -> Image.Image:
        """
        å¯è§†åŒ–æ£€æµ‹ç»“æœ
        
        Args:
            image: åŸå§‹å›¾åƒ
            detection_result: æ£€æµ‹ç»“æœ
        
        Returns:
            æ ‡æ³¨åçš„å›¾åƒ
        """
        draw = ImageDraw.Draw(image)
        
        # å°è¯•åŠ è½½å­—ä½“
        try:
            font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 16)
        except:
            font = ImageFont.load_default()
        
        # è§£ææ£€æµ‹ç»“æœ
        if 'bboxes' in detection_result and 'labels' in detection_result:
            bboxes = detection_result['bboxes']
            labels = detection_result['labels']
            
            # ä¸ºæ¯ä¸ªç±»åˆ«åˆ†é…é¢œè‰²
            colors = [
                'red', 'green', 'blue', 'yellow', 'purple', 
                'cyan', 'magenta', 'orange', 'pink', 'lime'
            ]
            
            for i, (bbox, label) in enumerate(zip(bboxes, labels)):
                color = colors[i % len(colors)]
                
                # ç”»è¾¹ç•Œæ¡†
                draw.rectangle(bbox, outline=color, width=2)
                
                # ç”»æ ‡ç­¾èƒŒæ™¯
                text_bbox = draw.textbbox((0, 0), label, font=font)
                text_width = text_bbox[2] - text_bbox[0]
                text_height = text_bbox[3] - text_bbox[1]
                
                label_bg = [bbox[0], bbox[1] - text_height - 4, 
                           bbox[0] + text_width + 4, bbox[1]]
                draw.rectangle(label_bg, fill=color)
                
                # ç”»æ ‡ç­¾æ–‡æœ¬
                draw.text((bbox[0] + 2, bbox[1] - text_height - 2), 
                         label, fill='white', font=font)
        
        return image
    
    def test_all_tasks(self, image_path: str, output_dir: str = None):
        """
        æµ‹è¯•æ‰€æœ‰æ”¯æŒçš„ä»»åŠ¡
        
        Args:
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        """
        print(f"\nğŸ“¸ æµ‹è¯•å›¾åƒ: {image_path}")
        print("=" * 60)
        
        # åŠ è½½å›¾åƒ
        image = Image.open(image_path).convert('RGB')
        print(f"å›¾åƒå°ºå¯¸: {image.size}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if output_dir is None:
            output_dir = Path("/workspace/output/florence2")
        else:
            output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True, parents=True)
        
        # æµ‹è¯•ç»“æœ
        results = {
            'image': str(image_path),
            'image_size': image.size,
            'tasks': {}
        }
        
        # å®šä¹‰è¦æµ‹è¯•çš„ä»»åŠ¡
        test_tasks = [
            ('caption', 'ğŸ“ å›¾åƒæè¿°'),
            ('detailed_caption', 'ğŸ“„ è¯¦ç»†æè¿°'),
            ('ocr', 'ğŸ“– æ–‡å­—è¯†åˆ«(OCR)'),
            ('ocr_with_region', 'ğŸ“ å¸¦åŒºåŸŸçš„OCR'),
            ('object_detection', 'ğŸ¯ ç›®æ ‡æ£€æµ‹'),
            ('dense_region_caption', 'ğŸ—ºï¸ å¯†é›†åŒºåŸŸæè¿°'),
        ]
        
        # æ‰§è¡Œæ¯ä¸ªä»»åŠ¡
        for task_name, task_desc in test_tasks:
            print(f"\n{task_desc} ({task_name})...")
            print("-" * 40)
            
            result = self.run_task(image, task_name)
            results['tasks'][task_name] = result
            
            if result['success']:
                print(f"âœ… æˆåŠŸ (ç”¨æ—¶: {result['time']:.2f}ç§’)")
                
                # æ‰“å°ç»“æœé¢„è§ˆ
                if 'parsed_output' in result:
                    output = result['parsed_output']
                    
                    # æ ¹æ®ä»»åŠ¡ç±»å‹æ˜¾ç¤ºç»“æœ
                    if task_name in ['caption', 'detailed_caption', 'more_detailed_caption']:
                        # æ–‡æœ¬æè¿°ç±»ä»»åŠ¡
                        if isinstance(output, dict) and task_name in output:
                            text = output[task_name]
                        else:
                            text = str(output)
                        print(f"ç»“æœ: {text[:200]}..." if len(text) > 200 else f"ç»“æœ: {text}")
                        
                    elif task_name == 'ocr':
                        # OCRä»»åŠ¡
                        if isinstance(output, dict) and 'OCR' in output:
                            text = output['OCR']
                        else:
                            text = str(output)
                        lines = text.split('\n')[:5]  # æ˜¾ç¤ºå‰5è¡Œ
                        for line in lines:
                            print(f"  {line[:80]}..." if len(line) > 80 else f"  {line}")
                        if len(text.split('\n')) > 5:
                            print(f"  ... (å…± {len(text.split('\n'))} è¡Œ)")
                            
                    elif task_name == 'object_detection':
                        # ç›®æ ‡æ£€æµ‹ä»»åŠ¡
                        if isinstance(output, dict) and 'bboxes' in output:
                            n_objects = len(output['bboxes'])
                            print(f"æ£€æµ‹åˆ° {n_objects} ä¸ªå¯¹è±¡")
                            if 'labels' in output:
                                # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ•°é‡
                                from collections import Counter
                                label_counts = Counter(output['labels'])
                                for label, count in label_counts.most_common(5):
                                    print(f"  - {label}: {count} ä¸ª")
                                    
                            # ä¿å­˜å¯è§†åŒ–ç»“æœ
                            vis_image = self.visualize_detection(image.copy(), output)
                            vis_path = output_dir / f"{Path(image_path).stem}_detection.png"
                            vis_image.save(vis_path)
                            print(f"  å¯è§†åŒ–ç»“æœå·²ä¿å­˜: {vis_path}")
                            
                    elif task_name == 'dense_region_caption':
                        # å¯†é›†åŒºåŸŸæè¿°
                        if isinstance(output, dict):
                            if 'bboxes' in output and 'labels' in output:
                                n_regions = len(output['bboxes'])
                                print(f"ç”Ÿæˆäº† {n_regions} ä¸ªåŒºåŸŸæè¿°")
                                # æ˜¾ç¤ºå‰3ä¸ªæè¿°
                                for i, label in enumerate(output['labels'][:3]):
                                    print(f"  åŒºåŸŸ{i+1}: {label[:60]}...")
                                    
            else:
                print(f"âŒ å¤±è´¥: {result.get('error', 'Unknown error')}")
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        results_file = output_dir / f"{Path(image_path).stem}_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self.print_stats()
        
        return results
    
    def interactive_vqa(self, image_path: str):
        """
        äº¤äº’å¼è§†è§‰é—®ç­”
        
        Args:
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
        """
        print(f"\nğŸ¤– è§†è§‰é—®ç­”æ¨¡å¼")
        print(f"å›¾åƒ: {image_path}")
        print("è¾“å…¥ 'quit' é€€å‡º\n")
        
        # åŠ è½½å›¾åƒ
        image = Image.open(image_path).convert('RGB')
        
        while True:
            question = input("â“ è¯·æé—®: ").strip()
            
            if question.lower() in ['quit', 'exit', 'q']:
                break
            
            if not question:
                continue
            
            # æ‰§è¡ŒVQA
            result = self.run_task(image, 'vqa', question)
            
            if result['success']:
                answer = result['parsed_output']
                if isinstance(answer, dict):
                    answer = answer.get('VQA', str(answer))
                print(f"ğŸ’¡ å›ç­”: {answer}\n")
            else:
                print(f"âŒ é”™è¯¯: {result.get('error', 'Unknown error')}\n")
    
    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "=" * 60)
        print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  æ€»ä»»åŠ¡æ•°: {self.stats['total_tasks']}")
        print(f"  æˆåŠŸ: {self.stats['successful_tasks']}")
        print(f"  å¤±è´¥: {self.stats['failed_tasks']}")
        if self.stats['successful_tasks'] > 0:
            avg_time = self.stats['total_time'] / self.stats['successful_tasks']
            print(f"  å¹³å‡å¤„ç†æ—¶é—´: {avg_time:.2f} ç§’/ä»»åŠ¡")
        print("=" * 60)

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Florence-2 å¤šåŠŸèƒ½è§†è§‰æ¨¡å‹æµ‹è¯•')
    parser.add_argument('image', help='è¾“å…¥å›¾åƒè·¯å¾„')
    parser.add_argument('--task', choices=list(Florence2Tester.TASK_PROMPTS.keys()),
                       help='æŒ‡å®šä»»åŠ¡ç±»å‹ï¼Œä¸æŒ‡å®šåˆ™æµ‹è¯•æ‰€æœ‰ä»»åŠ¡')
    parser.add_argument('--model', default='microsoft/Florence-2-large',
                       choices=['microsoft/Florence-2-base', 'microsoft/Florence-2-large'],
                       help='æ¨¡å‹ç‰ˆæœ¬')
    parser.add_argument('--output', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--vqa', action='store_true', help='è¿›å…¥äº¤äº’å¼VQAæ¨¡å¼')
    parser.add_argument('--text', help='é¢å¤–çš„æ–‡æœ¬è¾“å…¥ï¼ˆç”¨äºVQAç­‰ï¼‰')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥å›¾åƒæ–‡ä»¶
    if not Path(args.image).exists():
        print(f"âŒ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {args.image}")
        return
    
    # åˆ›å»ºæµ‹è¯•å™¨
    print("ğŸš€ Florence-2 è§†è§‰æ¨¡å‹æµ‹è¯•å·¥å…·")
    print("=" * 60)
    tester = Florence2Tester(model_id=args.model)
    
    # æ‰§è¡Œæµ‹è¯•
    if args.vqa:
        # äº¤äº’å¼VQAæ¨¡å¼
        tester.interactive_vqa(args.image)
    elif args.task:
        # æµ‹è¯•å•ä¸ªä»»åŠ¡
        image = Image.open(args.image).convert('RGB')
        result = tester.run_task(image, args.task, args.text or "")
        
        if result['success']:
            print(f"\nâœ… ä»»åŠ¡æˆåŠŸ: {args.task}")
            print(f"ç»“æœ: {json.dumps(result['parsed_output'], indent=2, ensure_ascii=False)}")
        else:
            print(f"\nâŒ ä»»åŠ¡å¤±è´¥: {result.get('error', 'Unknown error')}")
    else:
        # æµ‹è¯•æ‰€æœ‰ä»»åŠ¡
        tester.test_all_tasks(args.image, args.output)

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) == 1:
        # äº¤äº’å¼ç•Œé¢
        print("ğŸŒŸ Florence-2 å¤šåŠŸèƒ½è§†è§‰æ¨¡å‹æµ‹è¯•å·¥å…·")
        print("=" * 60)
        print("Florence-2 æ”¯æŒä»¥ä¸‹ä»»åŠ¡:")
        print("  â€¢ OCR - æ–‡å­—è¯†åˆ«")
        print("  â€¢ å›¾åƒæè¿° - ç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°")
        print("  â€¢ ç›®æ ‡æ£€æµ‹ - æ£€æµ‹å’Œå®šä½å¯¹è±¡")
        print("  â€¢ è§†è§‰é—®ç­” - å›ç­”å…³äºå›¾åƒçš„é—®é¢˜")
        print("  â€¢ åŒºåŸŸæè¿° - æè¿°å›¾åƒä¸­çš„ç‰¹å®šåŒºåŸŸ")
        print()
        
        image_path = input("è¯·è¾“å…¥å›¾åƒè·¯å¾„: ").strip()
        if image_path:
            tester = Florence2Tester()
            
            print("\né€‰æ‹©æ¨¡å¼:")
            print("1. æµ‹è¯•æ‰€æœ‰ä»»åŠ¡")
            print("2. äº¤äº’å¼é—®ç­”")
            print("3. åªåšOCR")
            
            choice = input("è¯·é€‰æ‹© (1/2/3): ").strip()
            
            if choice == '2':
                tester.interactive_vqa(image_path)
            elif choice == '3':
                image = Image.open(image_path).convert('RGB')
                result = tester.run_task(image, 'ocr')
                if result['success']:
                    print("\nOCRç»“æœ:")
                    print(result['parsed_output'].get('OCR', result['parsed_output']))
            else:
                tester.test_all_tasks(image_path)
        else:
            print("æœªè¾“å…¥è·¯å¾„ï¼Œé€€å‡ºç¨‹åº")
    else:
        main()
